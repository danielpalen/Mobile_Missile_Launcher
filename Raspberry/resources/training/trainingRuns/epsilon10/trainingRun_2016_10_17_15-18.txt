
#########################################################
Episode 1

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:           0           0           0           0           0           0           0
  badPosition:           0           0           0           0           0           0           0
        toFar:           0           0           0           0           0           0           0
 goodPosition:           0           0           0           0           0           0           0
choosing greedy
 [state     noTarget] [action     left] [reward  -3] [n_ state     noTarget] [t_reward  -3]

choosing random
 [state     noTarget] [action     fire] [reward -20] [n_ state     noTarget] [t_reward -23]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:        -0.3           0           0           0           0           0          -2
  badPosition:           0           0           0           0           0           0           0
        toFar:           0           0           0           0           0           0           0
 goodPosition:           0           0           0           0           0           0           0

#########################################################
Episode 2

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:        -0.3           0           0           0           0           0          -2
  badPosition:           0           0           0           0           0           0           0
        toFar:           0           0           0           0           0           0           0
 goodPosition:           0           0           0           0           0           0           0
choosing greedy
 [state     noTarget] [action    right] [reward  -3] [n_ state     noTarget] [t_reward  -3]

choosing greedy
 [state     noTarget] [action   toward] [reward  -5] [n_ state     noTarget] [t_reward  -8]

choosing greedy
 [state     noTarget] [action  forward] [reward  -2] [n_ state     noTarget] [t_reward -10]

choosing greedy
 [state     noTarget] [action backward] [reward  -2] [n_ state     noTarget] [t_reward -12]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -13]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward -14]

choosing greedy
 [state  badPosition] [action     left] [reward  -1] [n_ state     noTarget] [t_reward -15]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -16]

choosing greedy
 [state     noTarget] [action  forward] [reward  -2] [n_ state     noTarget] [t_reward -18]

choosing greedy
 [state     noTarget] [action backward] [reward  -2] [n_ state     noTarget] [t_reward -20]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -21]

choosing greedy
 [state     noTarget] [action     left] [reward  -3] [n_ state     noTarget] [t_reward -24]

choosing greedy
 [state     noTarget] [action    right] [reward  -3] [n_ state     noTarget] [t_reward -27]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward -28]

choosing greedy
 [state  badPosition] [action    right] [reward  -1] [n_ state  badPosition] [t_reward -29]

choosing greedy
-78.8595 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward -26]

choosing greedy
 [state        toFar] [action     left] [reward  -3] [n_ state     noTarget] [t_reward -29]

choosing greedy
 [state     noTarget] [action  forward] [reward  -2] [n_ state  badPosition] [t_reward -31]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:      -0.573      -0.573        -0.5     -0.5408      -0.382   -0.413505          -2
  badPosition:     -0.1019        -0.1         0.3           0           0           0           0
        toFar:    -0.30382           0           0           0           0           0           0
 goodPosition:           0           0           0           0           0           0           0

#########################################################
Episode 3

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:      -0.573      -0.573        -0.5     -0.5408      -0.382   -0.413505          -2
  badPosition:     -0.1019        -0.1         0.3           0           0           0           0
        toFar:    -0.30382           0           0           0           0           0           0
 goodPosition:           0           0           0           0           0           0           0
choosing greedy
 [state     noTarget] [action backward] [reward  -2] [n_ state     noTarget] [t_reward  -2]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -3]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -4]

choosing greedy
 [state     noTarget] [action   toward] [reward  -5] [n_ state     noTarget] [t_reward  -9]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -10]

choosing greedy
 [state     noTarget] [action  forward] [reward  -2] [n_ state     noTarget] [t_reward -12]

choosing greedy
 [state     noTarget] [action backward] [reward  -2] [n_ state     noTarget] [t_reward -14]

choosing greedy
 [state     noTarget] [action     left] [reward  -3] [n_ state     noTarget] [t_reward -17]

choosing greedy
 [state     noTarget] [action    right] [reward  -3] [n_ state     noTarget] [t_reward -20]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -21]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward -22]

choosing random
 [state  badPosition] [action  forward] [reward   0] [n_ state  badPosition] [t_reward -22]

choosing greedy
-233.842 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward -19]

choosing greedy
 [state        toFar] [action    right] [reward  -3] [n_ state  badPosition] [t_reward -22]

choosing random
 [state  badPosition] [action     left] [reward  -1] [n_ state     noTarget] [t_reward -23]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -24]

choosing greedy
 [state     noTarget] [action  forward] [reward  -2] [n_ state     noTarget] [t_reward -26]

choosing greedy
 [state     noTarget] [action backward] [reward  -2] [n_ state     noTarget] [t_reward -28]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -29]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward -30]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -0.82143    -0.82143      -0.955   -0.829836   -0.835484   -0.762902          -2
  badPosition:   -0.198375        -0.1        0.57       0.003           0           0           0
        toFar:    -0.30382     -0.2943           0           0           0           0           0
 goodPosition:           0           0           0           0           0           0           0

#########################################################
Episode 4

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -0.82143    -0.82143      -0.955   -0.829836   -0.835484   -0.762902          -2
  badPosition:   -0.198375        -0.1        0.57       0.003           0           0           0
        toFar:    -0.30382     -0.2943           0           0           0           0           0
 goodPosition:           0           0           0           0           0           0           0
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing greedy
 [state     noTarget] [action     left] [reward  -3] [n_ state     noTarget] [t_reward  -5]

choosing random
 [state     noTarget] [action   toward] [reward  -5] [n_ state     noTarget] [t_reward -10]

choosing greedy
 [state     noTarget] [action    right] [reward  -3] [n_ state     noTarget] [t_reward -13]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -14]

choosing greedy
 [state     noTarget] [action  forward] [reward  -2] [n_ state     noTarget] [t_reward -16]

choosing greedy
 [state     noTarget] [action backward] [reward  -2] [n_ state     noTarget] [t_reward -18]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -19]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -20]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -21]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -22]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward -23]

choosing random
 [state  badPosition] [action     fire] [reward -20] [n_ state  badPosition] [t_reward -43]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:     -1.0475     -1.0475    -1.36771   -0.955151    -0.96029   -0.932353          -2
  badPosition:   -0.198375        -0.1        0.57       0.003           0           0     -1.9943
        toFar:    -0.30382     -0.2943           0           0           0           0           0
 goodPosition:           0           0           0           0           0           0           0

#########################################################
Episode 5

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:     -1.0475     -1.0475    -1.36771   -0.955151    -0.96029   -0.932353          -2
  badPosition:   -0.198375        -0.1        0.57       0.003           0           0     -1.9943
        toFar:    -0.30382     -0.2943           0           0           0           0           0
 goodPosition:           0           0           0           0           0           0           0
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing greedy
 [state     noTarget] [action  forward] [reward  -2] [n_ state  badPosition] [t_reward  -4]

choosing greedy
-118.145 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward  -1]

choosing greedy
-31.1141 pixels to the left
 [state        toFar] [action   toward] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing random
 [state     noTarget] [action     left] [reward  -3] [n_ state  badPosition] [t_reward  -5]

choosing greedy
137.264 pixels to the right
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward  -2]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state        toFar] [t_reward   1]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward   4]

choosing greedy
 [state goodPosition] [action     left] [reward  -3] [n_ state  badPosition] [t_reward   1]

choosing greedy
249.853 pixels to the right
 [state  badPosition] [action   toward] [reward   3] [n_ state goodPosition] [t_reward   4]

choosing greedy
 [state goodPosition] [action    right] [reward  -3] [n_ state  badPosition] [t_reward   1]

choosing greedy
-187.094 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state goodPosition] [t_reward   4]

choosing greedy
16.1526 pixels to the right
 [state goodPosition] [action   toward] [reward  -1] [n_ state        toFar] [t_reward   3]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward   6]

choosing greedy
 [state goodPosition] [action  forward] [reward  -1] [n_ state     noTarget] [t_reward   5]

choosing greedy
 [state     noTarget] [action backward] [reward  -2] [n_ state goodPosition] [t_reward   3]

choosing greedy
 [state goodPosition] [action backward] [reward  -2] [n_ state        toFar] [t_reward   1]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward   4]

choosing greedy
l
 [state goodPosition] [action   search] [reward  -3] [n_ state     noTarget] [t_reward   1]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward   0]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -3]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -4]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -5]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -6]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -7]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -8]

choosing greedy
 [state     noTarget] [action    right] [reward  -3] [n_ state     noTarget] [t_reward -11]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -12]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -13]

choosing greedy
 [state     noTarget] [action  forward] [reward  -2] [n_ state     noTarget] [t_reward -15]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -16]

choosing random
 [state     noTarget] [action  forward] [reward  -2] [n_ state     noTarget] [t_reward -18]

choosing random
 [state     noTarget] [action     fire] [reward -20] [n_ state     noTarget] [t_reward -38]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.25322    -1.36771    -1.25381    -1.06426    -1.06338    -3.81063
  badPosition:   -0.198375        -0.1     1.40568       0.003           0           0     -1.9943
        toFar:    -0.30382     -0.2943   -0.109603      1.0317           0           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631           0

#########################################################
Episode 6

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.25322    -1.36771    -1.25381    -1.06426    -1.06338    -3.81063
  badPosition:   -0.198375        -0.1     1.40568       0.003           0           0     -1.9943
        toFar:    -0.30382     -0.2943   -0.109603      1.0317           0           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631           0
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
 [state     noTarget] [action backward] [reward  -2] [n_ state     noTarget] [t_reward  -3]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -4]

choosing random
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -5]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -6]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward  -7]

choosing greedy
-150.444 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state     noTarget] [t_reward  -4]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -5]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -6]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward  -7]

choosing greedy
-182.385 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward  -4]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state        toFar] [t_reward  -1]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state  badPosition] [t_reward   2]

choosing greedy
-90.5172 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward   5]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward   8]

choosing greedy
 [state goodPosition] [action     fire] [reward  10] [n_ state goodPosition] [t_reward  18]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.25322    -1.36771    -1.25381    -1.16848    -1.04374    -3.81063
  badPosition:   -0.198375        -0.1     1.85279       0.003           0           0     -1.9943
        toFar:    -0.30382     -0.2943   -0.109603     1.58885           0           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631           1

#########################################################
Episode 7

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.25322    -1.36771    -1.25381    -1.16848    -1.04374    -3.81063
  badPosition:   -0.198375        -0.1     1.85279       0.003           0           0     -1.9943
        toFar:    -0.30382     -0.2943   -0.109603     1.58885           0           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631           1
choosing random
 [state     noTarget] [action  forward] [reward  -2] [n_ state     noTarget] [t_reward  -2]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -3]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -4]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward  -5]

choosing greedy
232.741 pixels to the right
 [state  badPosition] [action   toward] [reward   3] [n_ state     noTarget] [t_reward  -2]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state        toFar] [t_reward  -3]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state        toFar] [t_reward   0]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward   3]

choosing greedy
 [state goodPosition] [action     fire] [reward  10] [n_ state goodPosition] [t_reward  13]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.25322    -1.36771    -1.33887    -1.16848    -1.01225    -3.81063
  badPosition:   -0.198375        -0.1      1.9572       0.003           0           0     -1.9943
        toFar:    -0.30382     -0.2943   -0.109603     1.88127           0           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631        1.91

#########################################################
Episode 8

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.25322    -1.36771    -1.33887    -1.16848    -1.01225    -3.81063
  badPosition:   -0.198375        -0.1      1.9572       0.003           0           0     -1.9943
        toFar:    -0.30382     -0.2943   -0.109603     1.88127           0           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631        1.91
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -3]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -4]

choosing random
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -5]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -6]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -7]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -8]

choosing random
 [state     noTarget] [action backward] [reward  -2] [n_ state     noTarget] [t_reward -10]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -11]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -12]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward -13]

choosing greedy
-116.056 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state     noTarget] [t_reward -10]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -11]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -12]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward -13]

choosing greedy
-255.668 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward -10]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state  badPosition] [t_reward  -7]

choosing greedy
-70.8771 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward  -4]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward  -1]

choosing greedy
 [state goodPosition] [action     fire] [reward  10] [n_ state goodPosition] [t_reward   9]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.25322    -1.36771    -1.33887    -1.26228     -1.0308    -3.81063
  badPosition:   -0.198375        -0.1     2.26841       0.003           0           0     -1.9943
        toFar:    -0.30382     -0.2943   -0.109603     2.13241           0           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631      2.7381

#########################################################
Episode 9

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.25322    -1.36771    -1.33887    -1.26228     -1.0308    -3.81063
  badPosition:   -0.198375        -0.1     2.26841       0.003           0           0     -1.9943
        toFar:    -0.30382     -0.2943   -0.109603     2.13241           0           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631      2.7381
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing random
 [state     noTarget] [action backward] [reward  -2] [n_ state     noTarget] [t_reward  -4]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -5]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -6]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -7]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -8]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -9]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -10]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -11]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -12]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward -13]

choosing greedy
-76.5602 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state     noTarget] [t_reward -10]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -11]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -12]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward -13]

choosing greedy
-288.641 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward -10]

choosing random
 [state        toFar] [action    right] [reward  -3] [n_ state  badPosition] [t_reward -13]

choosing greedy
-162.199 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward -10]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state  badPosition] [t_reward  -7]

choosing greedy
-61.313 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward  -4]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward  -1]

choosing greedy
 [state goodPosition] [action     fire] [reward  10] [n_ state goodPosition] [t_reward   9]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.25322    -1.36771    -1.33887     -1.3465    -1.03052    -3.81063
  badPosition:   -0.198375        -0.1     2.57126       0.003           0           0     -1.9943
        toFar:    -0.30382   -0.540677   -0.109603     2.34712           0           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     3.49167

#########################################################
Episode 10

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.25322    -1.36771    -1.33887     -1.3465    -1.03052    -3.81063
  badPosition:   -0.198375        -0.1     2.57126       0.003           0           0     -1.9943
        toFar:    -0.30382   -0.540677   -0.109603     2.34712           0           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     3.49167
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -3]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -4]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward  -5]

choosing random
 [state  badPosition] [action  forward] [reward   0] [n_ state  badPosition] [t_reward  -5]

choosing greedy
-191.107 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward  -2]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state        toFar] [t_reward   1]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward   4]

choosing greedy
 [state goodPosition] [action     fire] [reward  10] [n_ state goodPosition] [t_reward  14]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.25322    -1.36771    -1.33887     -1.3465    -1.02455    -3.81063
  badPosition:   -0.198375        -0.1     2.63761   0.0284126           0           0     -1.9943
        toFar:    -0.30382   -0.540677   -0.109603     2.52721           0           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     4.17742

#########################################################
Episode 11

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.25322    -1.36771    -1.33887     -1.3465    -1.02455    -3.81063
  badPosition:   -0.198375        -0.1     2.63761   0.0284126           0           0     -1.9943
        toFar:    -0.30382   -0.540677   -0.109603     2.52721           0           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     4.17742
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -3]

choosing random
 [state     noTarget] [action    right] [reward  -3] [n_ state     noTarget] [t_reward  -6]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -7]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -8]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -9]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -10]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -11]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state        toFar] [t_reward -12]

choosing random
 [state        toFar] [action     left] [reward  -3] [n_ state  badPosition] [t_reward -15]

choosing greedy
147.463 pixels to the right
 [state  badPosition] [action   toward] [reward   3] [n_ state     noTarget] [t_reward -12]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state        toFar] [t_reward -13]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state        toFar] [t_reward -10]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward  -7]

choosing greedy
 [state goodPosition] [action     fire] [reward  10] [n_ state goodPosition] [t_reward   3]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.3465    -1.00901    -3.81063
  badPosition:   -0.198375        -0.1     2.66347   0.0284126           0           0     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     2.68156           0           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     4.80145

#########################################################
Episode 12

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.3465    -1.00901    -3.81063
  badPosition:   -0.198375        -0.1     2.66347   0.0284126           0           0     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     2.68156           0           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     4.80145
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -3]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -4]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -5]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -6]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -7]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -8]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -9]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward -10]

choosing greedy
161.083 pixels to the right
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward  -7]

choosing random
 [state        toFar] [action backward] [reward  -2] [n_ state        toFar] [t_reward  -9]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state        toFar] [t_reward  -6]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward  -3]

choosing greedy
 [state goodPosition] [action     fire] [reward  10] [n_ state goodPosition] [t_reward   7]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.3465    -1.03404    -3.81063
  badPosition:   -0.198375        -0.1     2.72394   0.0284126           0           0     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     2.81421   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     5.36932

#########################################################
Episode 13

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.3465    -1.03404    -3.81063
  badPosition:   -0.198375        -0.1     2.72394   0.0284126           0           0     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     2.81421   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     5.36932
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing random
 [state     noTarget] [action     fire] [reward -20] [n_ state     noTarget] [t_reward -21]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.3465    -1.04098    -5.43998
  badPosition:   -0.198375        -0.1     2.72394   0.0284126           0           0     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     2.81421   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     5.36932

#########################################################
Episode 14

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.3465    -1.04098    -5.43998
  badPosition:   -0.198375        -0.1     2.72394   0.0284126           0           0     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     2.81421   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     5.36932
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -3]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -4]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -5]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -6]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -7]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -8]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward  -9]

choosing random
f
 [state  badPosition] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -10]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward -11]

choosing greedy
-314.26 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward  -8]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward  -5]

choosing greedy
 [state goodPosition] [action     fire] [reward  10] [n_ state goodPosition] [t_reward   5]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.3465    -1.01153    -5.43998
  badPosition:   -0.198375        -0.1     2.77969   0.0284126           0   -0.110431     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     2.88648   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     5.88608

#########################################################
Episode 15

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.3465    -1.01153    -5.43998
  badPosition:   -0.198375        -0.1     2.77969   0.0284126           0   -0.110431     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     2.88648   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     5.88608
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -3]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -4]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward  -5]

choosing greedy
-203.152 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state     noTarget] [t_reward  -2]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -3]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -4]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward  -5]

choosing greedy
-242.616 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward  -2]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state  badPosition] [t_reward   1]

choosing greedy
-71.6058 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward   4]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state  badPosition] [t_reward   7]

choosing greedy
-81.0847 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state goodPosition] [t_reward  10]

choosing greedy
 [state goodPosition] [action     fire] [reward  10] [n_ state goodPosition] [t_reward  20]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.3465   -0.997281    -5.43998
  badPosition:   -0.198375        -0.1     2.95666   0.0284126           0   -0.110431     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     2.96249   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     6.35633

#########################################################
Episode 16

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.3465   -0.997281    -5.43998
  badPosition:   -0.198375        -0.1     2.95666   0.0284126           0   -0.110431     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     2.96249   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     6.35633
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward  -2]

choosing greedy
-164.688 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state     noTarget] [t_reward   1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward   0]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state        toFar] [t_reward  -2]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state        toFar] [t_reward   1]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state        toFar] [t_reward   4]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward   7]

choosing greedy
 [state goodPosition] [action     fire] [reward  10] [n_ state goodPosition] [t_reward  17]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.3465   -0.970577    -5.43998
  badPosition:   -0.198375        -0.1     2.95122   0.0284126           0   -0.110431     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     3.08718   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     6.78426

#########################################################
Episode 17

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.3465   -0.970577    -5.43998
  badPosition:   -0.198375        -0.1     2.95122   0.0284126           0   -0.110431     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     3.08718   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     6.78426
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing random
 [state     noTarget] [action backward] [reward  -2] [n_ state     noTarget] [t_reward  -4]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -5]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -6]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -7]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -8]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -9]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward -10]

choosing greedy
-110.796 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state  badPosition] [t_reward  -7]

choosing greedy
-51.6111 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state     noTarget] [t_reward  -4]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -5]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -6]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state        toFar] [t_reward  -7]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state        toFar] [t_reward  -4]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward  -1]

choosing greedy
 [state goodPosition] [action     fire] [reward  10] [n_ state goodPosition] [t_reward   9]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.4218    -0.99014    -5.43998
  badPosition:   -0.198375        -0.1       2.977   0.0284126           0   -0.110431     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     3.16624   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     7.17368

#########################################################
Episode 18

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.4218    -0.99014    -5.43998
  badPosition:   -0.198375        -0.1       2.977   0.0284126           0   -0.110431     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     3.16624   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     7.17368
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -3]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -4]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward  -5]

choosing greedy
-155.349 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward  -2]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state        toFar] [t_reward   1]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state  badPosition] [t_reward   4]

choosing random
 [state  badPosition] [action backward] [reward   0] [n_ state        toFar] [t_reward   4]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state  badPosition] [t_reward   7]

choosing greedy
-100.294 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward  10]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward  13]

choosing greedy
 [state goodPosition] [action     fire] [reward  10] [n_ state goodPosition] [t_reward  23]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.4218    -0.99557    -5.43998
  badPosition:   -0.198375        -0.1     3.04191   0.0284126   0.0319326   -0.110431     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     3.25538   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     7.52805

#########################################################
Episode 19

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.4218    -0.99557    -5.43998
  badPosition:   -0.198375        -0.1     3.04191   0.0284126   0.0319326   -0.110431     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     3.25538   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     7.52805
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -3]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -4]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state        toFar] [t_reward  -5]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state  badPosition] [t_reward  -2]

choosing greedy
-81.0861 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward   1]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state  badPosition] [t_reward   4]

choosing greedy
-85.9068 pixels to the left
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward   7]

choosing random
 [state        toFar] [action     left] [reward  -3] [n_ state  badPosition] [t_reward   4]

choosing greedy
123.769 pixels to the right
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward   7]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward  10]

choosing random
r
 [state goodPosition] [action   search] [reward  -3] [n_ state     noTarget] [t_reward   7]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward   6]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward   5]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward   4]

choosing random
 [state     noTarget] [action     left] [reward  -3] [n_ state     noTarget] [t_reward   1]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward   0]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -3]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -4]

choosing random
 [state     noTarget] [action   toward] [reward  -5] [n_ state     noTarget] [t_reward  -9]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -10]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -11]

choosing random
 [state     noTarget] [action     left] [reward  -3] [n_ state     noTarget] [t_reward -14]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -15]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -16]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -17]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -18]

choosing random
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -19]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -20]

choosing random
 [state     noTarget] [action     fire] [reward  10] [n_ state     noTarget] [t_reward -40]

#########################################################
Episode 20

Reading properties.txt file:
Launcher opened
VehicleController: Arduino connected!
starting reinforcement learning

read qValues from file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887     -1.4218    -0.99557    -5.43998
  badPosition:   -0.198375        -0.1     3.04191   0.0284126   0.0319326   -0.110431     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     3.25538   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     7.52805
choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -1]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -2]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -3]

choosing random
 [state     noTarget] [action backward] [reward  -2] [n_ state     noTarget] [t_reward  -5]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -6]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -7]

choosing greedy
r
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -8]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward  -9]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -10]

choosing greedy
l
 [state     noTarget] [action   search] [reward  -1] [n_ state     noTarget] [t_reward -11]

choosing greedy
f
 [state     noTarget] [action   search] [reward  -1] [n_ state  badPosition] [t_reward -12]

choosing greedy
297.721 pixels to the right
 [state  badPosition] [action   toward] [reward   3] [n_ state        toFar] [t_reward  -9]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state        toFar] [t_reward  -6]

choosing greedy
 [state        toFar] [action  forward] [reward   3] [n_ state goodPosition] [t_reward  -3]

choosing greedy
 [state goodPosition] [action     fire] [reward  10] [n_ state goodPosition] [t_reward   7]

updated qValues written to file: ../resources/training/qValues.txt
  state/action        left       right      toward     forward    backward      search        fire
     noTarget:    -1.23462    -1.43836    -1.36771    -1.33887    -1.48986    -1.02508    -5.43998
  badPosition:   -0.198375        -0.1     3.07027   0.0284126   0.0319326   -0.110431     -1.9943
        toFar:   -0.547062   -0.540677   -0.109603     3.31144   -0.173184           0           0
 goodPosition:   -0.289683   -0.287715     -0.0943   -0.109603    -0.19187   -0.309631     7.85053
